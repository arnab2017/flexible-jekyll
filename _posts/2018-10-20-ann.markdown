---
layout: post
title: Artificial Neural Network (ANN)
date: 2018-10-22 
description: Here I discuss about Artificial Neural Network (ANN). # Add post description (optional)
img: ann-heading.jpg # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [ANN, Introduction to Artificial Neural Network, What is ANN, What is Feedforward Neural Network]
---

## Introduction to Artificial Neural Network (ANN):
<p align="justify">
An Artificial Neural Network (ANN) used in deep learning, consists of different layers connected to each other and it is a computational model that is inspired by the way biological neural networks in the human brain process information. It learns from huge volumes of data and uses complex algorithms to train a neural net. Artificial Neural Networks have generated a lot of excitement in Machine Learning research and industry, thanks to many breakthrough results in speech recognition, computer vision and text processing.
</p>

## Let’s Understand Single Neuron:
<p align="justify">
The neuron is the basic unit of computation in a neural network and it often called a node or unit. It receives input from some other nodes or from an external source and computes an output. Each input has an associated weight (w), which is assigned based on its relative importance to other inputs. The node applies a function f (defined below) to the weighted sum of its inputs as shown in the figure below.
</p>



![figure1]({{site.baseurl}}/assets/img/annimg/ann1.png)

>Figure 1: Figure of a single neuron

<p align="justify">
The above network takes numerical inputs like X1, X2 and has weights w1 and w2 associated with those inputs. There is another input 1 with weight b associated with it. It is called Bias (b). 

The output Y from the neuron is computed as shown in the figure 1. The function f is non-linear and is called the activation function. Activation function is used to introduce non-linearity into the output of a neuron. Every activation function takes a single number and performs a certain fixed mathematical operation on it. There are several types of activation functions we use.
</p>
<p align="justify">

## Sigmoid Function:

Sigmoid function also known as logistic function or logistic curve and it is look like “S” shape. The equation of sigmoid function is given below:
</p>
e1
<p align="justify">
In the sigmoid function equation “e” stands for Euler’s number, “x0” stands for the x-value of the sigmoid midpoint, L stands for the curve’s maximum value and k stands for the steepness of the curve. The main reason why we use sigmoid function is because it exists between 0 to 1. Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable. That means, we can find the slope of the sigmoid curve at any two points. The function is monotonic but function’s derivative is not. The logistic sigmoid function can cause a neural network to get stuck at the training time. The softmax function is a more generalized logistic activation function, which is used for multiclass classification. Figure of sigmoid function given below.
</p>
![figure2]({{site.baseurl}}/assets/img/annimg/ann2.png)

>Figure 2: Figure of sigmoid function


## Threshold Function:

<p align="justify">
A threshold function is a Boolean function that determines whether a value equality of its inputs exceeded a certain threshold. A device that implements such logic is known as a threshold gate. It’s also known as Step function or Heaviside step function. The equation of threshold function is given below:
</p>
e2
<p align="justify">
Using second equation, Let n Boolean inputs be x1,x2…xn. f is a function that decides whether w1x1 + w2x2 + … + wnxn = t where t is a real number called the threshold and w1, w2, .. wn are real-numbered weights. A threshold function on n Boolean inputs x1, x2, …, xn is then a Boolean function that decides whether w1x1 + w2x2 + … + wnxn ≥ t. Figure of threshold function given below.
</p>

![figure3]({{site.baseurl}}/assets/img/annimg/ann3.png)

>Figure 3: Figure of threshold function

## Rectifier Linear Unit Activation Function (ReLU):
<p align="justify">
ReLU is an activation function that defined as the positive part of its argument. The ReLU is the most used activation function in the world right now. Since, it is used in almost all the convolutional neural networks or deep learning. Its equation is:
</p>
e3

<p align="justify">
In the equation x the is input to a neuron. This is also known as a ramp function. Rectified linear units find applications in computer vision and speech recognition using deep neural nets. The function and its derivative both are monotonic. But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately. Figure of threshold function given below.
</p>

![figure4]({{site.baseurl}}/assets/img/annimg/ann4.png)

>Figure 4: Figure of ReLU activation function
<p align="justify">
Tanh or Hyperbolic Tangent Activation Function:
Tanh is also like logistic sigmoid but better. The range of the tanh function is from -1 to 1. Tanh is also sigmoidal s-shaped. Equation of Tanh is:
</p>

e4
<p align="justify">
The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. The function is differentiable. The function is monotonic while its derivative is not monotonic. The tanh function is mainly used classification between two classes. Both tanh and logistic sigmoid activation functions are used in feed-forward nets.
</p>

![figure5]({{site.baseurl}}/assets/img/annimg/ann5.jpeg)

>Figure 5: Figure of Tanh activation function with Sigmoid

## Let’s try to understand Bias:
<p align="justify">
A Bias neuron allows a classifier to shift the decision boundary left or right. In an algebraic term, the bias neuron allows a classifier to translate its decision boundary. To translation is to “move every point a constant distance in a specified direction”.

A bias unit is an “extra” neuron added to each pre-output layer that stores the value of 1. Bias units aren’t connected to any previous layer and in this sense don’t represent a true “activity”.
</p>
Take a look at the following illustration:

![figure6]({{site.baseurl}}/assets/img/annimg/ann6.png)

>Figure 6: Figure of Bias in layer

<p align="justify">
The bias units are characterized by the text “+1”. We can see, a bias unit is just appended to the end of the input and each hidden layer, and isn’t influenced by the values in the previous layer. In other words, these neurons don’t have any incoming connections.
</p>
<p align="justify">
Bias units still have outgoing connections and they can contribute to the output of the ANN. Let’s call the outgoing weights of the bias units w_b. Now, let’s look at a really simple neural network that just has one input and one connection:
</p>

![figure]({{site.baseurl}}/assets/img/annimg/ann7.png)

>Figure 7: Figure of Bias

Let’s say act()  our activation function   is just f(x) = x, or the identity function. In such case, our ANN would represent a line because the output is just the weight (m) times the input (x).

![figure8]({{site.baseurl}}/assets/img/annimg/ann8.gif)

>Figure 8: Figure of Bias

When we change our weight w1, we will change the gradient of the function to make it steeper or flatter.
<p align="justify">
So, we know that our function output = w · input (y = mx) needs to have this constant term added to it. In other words, we can say output = w · input + w_b, where w_b is our constant term c. When we use neural networks, though, or do any multi-variable learning, our computations will be done through Linear Algebra and matrix arithmetic eg. dot-product, multiplication. This can also be seen graphically in the ANN. There should be a matching number of weights and activities for a weighted sum to occur. Because of this, we need to “add” an extra input term so that we can add a constant term with it. Since, one multiplied by any value is that value, we just “insert” an extra value of 1 at every layer. This is called the bias unit.
</p>

![figure9]({{site.baseurl}}/assets/img/annimg/ann9.gif)

>Figure 9: Figure of Bias in layer
<p align="justify">
From this diagram, you can see that we have now added the bias term and hence the weight w_b will be added to the weighted sum, and fed through activation function as a constant value. This constant term, also called the “intercept term”, shifts the activation function to the left or to the right. It will also be the output when the input is zero. Here is a diagram of how different weights will transform the activation function (sigmoid in this case) by scaling it up/down.
</p>

![figure10]({{site.baseurl}}/assets/img/annimg/ann10.png)

Figure 10: Figure of Bias in sign waves

But now, by adding the bias unit, we the possibility of translating the activation function exists:

![figure11]({{site.baseurl}}/assets/img/annimg/ann11.png)

>Figure 11: Figure of Bias in sign waves
<p align="justify">
Going back to the linear regression example, if w_b = 1, then we will add bias · w_b= 1 · w_b = w_b to the activation function. In the example with the line, we can create a non-zero y-intercept:
</p>

![figure12]({{site.baseurl}}/assets/img/annimg/ann12.png)

>Figure 12: Figure of Bias
<p align="justify">
I’m sure you can imagine infinite scenarios where the line of best fit does not go through the origin or even come near it. Bias units are important with neural networks in the same way.
</p>

## How do Neural Network Work:
<p align="justify">
Consider that we have some parameters about the property. We have area in square feet, number of bedrooms, distance from the city and the age of the property and all of those four are going to comprise as the input layer. Now of course they are probably way more parameters that define the price of a property. It is a very basic form of a neural network and it only has an input layer and output layer. It has no hidden layers and our output layer is the price that we are predicting. So here inputs variables would just be weighted up by the synopses and then output would be calculated. Basically the price would be calculated and get a price. And for instance the price could be calculated as simple as the weighted sum of all of the inputs.
</p>

![figure13]({{site.baseurl}}/assets/img/annimg/ann13.png)
 
>Figure 13: Figure of Neural Network without hidden layer.

<p align="justify">
Now let’s make our network a little more complex. Here, and in all neural network diagrams, the layer on the far left is the input layer, and the layer on the far right is the output layer (the network’s prediction/answer). Any number of layers in between these two are known as hidden layers. The more the number of layers, the more nuanced the decision-making can get.
</p>

![figure14]({{site.baseurl}}/assets/img/annimg/ann14.png)

>Figure 14: Figure of Neural Network with hidden layer.

Now with the hidden layers artificial neural network follow some steps for work. All the steps are given below.

![figure15]({{site.baseurl}}/assets/img/annimg/ann15.PNG)
 
Figure 15: Figure of Steps of how Neural Network work.
<p align="justify">
The networks often go by different names: deep feedforward networks, feedforward neural networks, or multi-layer perceptron’s (MLP). Let’s talk about feed forward neural network.
</p>
## Feedforward Neural Network:
<p align="justify">
The feedforward neural network was the first and simplest type of artificial neural network devised. It contains multiple neurons (nodes) arranged in layers. Nodes from adjacent layers have connections or edges between them. All these connections have weights associated with them.
</p>

![figure16]({{site.baseurl}}/assets/img/annimg/ann16.png)
 
>Figure 16: Figure of Feed Forward Neural Network.

**A feedforward neural network can consist of three types of nodes:**

**1. Input Nodes:**
<p align="justify">
The Input nodes provide information from the outside world to the network and are together referred to as the “Input Layer”. No computation is performed in any of the Input nodes. They just pass on the information to the hidden nodes.
</p>
**2.	Hidden Nodes:**
<p align="justify">The Hidden nodes have no direct connection with the outside world (hence the name “hidden”). They perform computations and transfer information from the input nodes to the output nodes. A collection of hidden nodes forms a “Hidden Layer”. While a feedforward network will only have a single input layer and a single output layer, it can have zero or multiple Hidden Layers.
</p>
**3.	Output Nodes:**
<p align="justify">The Output nodes are collectively referred to as the “Output Layer” and are responsible for computations and transferring information from the network to the outside world.
</p>
<p align="justify">
In a feedforward network, the information moves in only one direction which is forward direction from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network. This property of feed forward networks is different from Recurrent Neural Networks in which the connections between the nodes form a cycle.
</p>

**Two examples of feedforward networks are given below:**

**1.	Single Layer Perceptron** – Single Layer Perceptron is the simplest feedforward neural network and does not contain any hidden layer.

**2.	Multi-Layer Perceptron** – A Multi-Layer Perceptron has one or more hidden layers. We will only discuss Multi-Layer Perceptrons below since they are more useful than Single Layer Perceptions for practical applications today.

<p align="justify">
A Multi-Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer).  While a single layer perceptron can only learn linear functions, a multi-layer perceptron can also learn non linear functions.
</p>
<p align="justify">
Figure 17 shows a multi-layer perceptron with a single hidden layer. Note that all connections have weights associated with them, but only three weights (w0, w1, w2) are shown in the figure.
</p>

**Input Layer:**
<p align="justify">
The Input layer has three nodes. The Bias node has a value of 1. The other two nodes take X1 and X2 as external inputs (which are numerical values depending upon the input dataset). As discussed above, no computation is performed in the Input layer, so the outputs from nodes in the Input layer are 1, X1 and X2 respectively, which are fed into the Hidden Layer.
</p>
**Hidden Layer:**
<p align="justify">
The Hidden layer also has three nodes with the Bias node having an output of 1. The output of the other two nodes in the Hidden layer depends on the outputs from the Input layer (1, X1, X2) as well as the weights associated with the connections (edges). Figure 17 shows the output calculation for one of the hidden nodes (highlighted). Similarly, the output from other hidden node can be calculated. Remember that f refers to the activation function. These outputs are then fed to the nodes in the Output layer.
</p>

![figure17]({{site.baseurl}}/assets/img/annimg/ann17.png)
 
>Figure 17: Figure of a Multi-Layer perceptron with a single hidden layer.

**Output Layer:**
<p align="justify">
The Output layer has two nodes which take inputs from the Hidden layer and perform similar computations as shown for the highlighted hidden node. The values calculated (Y1 and Y2) as a result of these computations act as outputs of the Multi-Layer Perceptron.
</p>
<p align="justify">
Given a set of features X = (x1, x2, …) and a target y, a Multi-Layer Perceptron can learn the relationship between the features and the target, for either classification or regression.
</p>
Let us take an example to understand Multi-Layer Perceptrons better. Suppose we have the following student marks dataset:

| Hours Studied | Mid Term Marks | Final Term Result | 
|:------------: | :------------: | :---------------: |
| 35 | 67 | 1(Pass) |
| 12 | 75 | 0(Fail) |
| 16 | 89 | 1(Pass) |
| 45 | 56 | 1(Pass) |
| 10 | 90 | 0(Fail) |

<p align="justify">
The two input columns show the number of hours the student has studied and the mid-term marks obtained by the student. The Final Result column can have two values 1 or 0 indicating whether the student passed in the final term. For example, we can see that if the student studied 35 hours and had obtained 67 marks in the mid-term, he or she ended up passing the final term.
Now, suppose, we want to predict whether a student studying 25 hours and having 70 marks in the mid term will pass the final term.
</p>

| Hours Studied | Mid Term Marks | Final Term Result | 
|:------------: | :------------: | :---------------: |
| 25 | 70 | ? |

<p align="justify">
This is a binary classification problem where a multi-layer perceptron can learn from the given examples (training data) and make an informed prediction given a new data point. We will see below how a multi-layer perceptron learns such relationships.
</p>

## Training Multi-Layer Perceptron:

<p align="justify">
The process by which a Multi-Layer Perceptron learns is called the Backpropagation algorithm. I would recommend reading this Quora answer (quoted below) which URL given below where explains Backpropagation clearly.
</p>

[[Link of Quora Answer](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri)]


>Backward Propagation of Errors, often abbreviated as BackProp is one of the several ways in which an artificial neural network (ANN) can be trained. It is a supervised training scheme, which means, it learns from labeled training data (there is a supervisor, to guide its learning).




